{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training already completed.  \n",
        "\n",
        "This notebook documents the training setup and logs.\n"
      ],
      "metadata": {
        "id": "jvu2PmTqaZhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install rasterio segmentation-models-pytorch==0.3.3 torchmetrics==1.3.0 albumentations opencv-python scipy tifffile\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gITz_8ysemVz",
        "outputId": "fd0923da-068a-49ae-dbd0-3be21c2b4f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'torchmetrics' candidate (version 1.3.0 at https://files.pythonhosted.org/packages/95/f4/07d76def72c02f0d93e5eec953fd3349b653af0e0b792276aeb5b3e6f7bf/torchmetrics-1.3.0-py3-none-any.whl (from https://pypi.org/simple/torchmetrics/) (requires-python:>=3.8))\n",
            "Reason for being yanked: <none given>\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-SvWkVhK2Uc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f77169b-c9b7-40ac-ec96-6ed2dd18fd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Core\n",
        "import os, json, time, random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "\n",
        "# Imaging / Geo\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "from tqdm import tqdm\n",
        "import tifffile\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from PIL import Image\n",
        "\n",
        "# DL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Augmentation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Models\n",
        "import segmentation_models_pytorch as smp\n",
        "import torchvision\n",
        "from torchvision.models.segmentation import DeepLabV3_ResNet50_Weights\n",
        "\n",
        "# Metrics\n",
        "import time\n",
        "from torchmetrics.classification import MulticlassF1Score, MulticlassJaccardIndex\n",
        "from torchmetrics.functional.classification import multiclass_accuracy\n",
        "from torchmetrics.classification import (\n",
        "    MulticlassF1Score,\n",
        "    MulticlassJaccardIndex,\n",
        "    MulticlassPrecision,\n",
        "    MulticlassRecall,\n",
        "    MulticlassAccuracy\n",
        ")\n",
        "\n",
        "# Reproducibility + device\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Val/Test Split**"
      ],
      "metadata": {
        "id": "y_ifxQa90cOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Paths ----\n",
        "TILES_BASE = Path(\"/content/drive/MyDrive/ResearchProject (1)/dataset_tiles_512\")\n",
        "IMG_DIR = TILES_BASE / \"images\"\n",
        "MSK_DIR = TILES_BASE / \"masks\"\n",
        "\n",
        "assert IMG_DIR.exists(), f\"Missing: {IMG_DIR}\"\n",
        "assert MSK_DIR.exists(), f\"Missing: {MSK_DIR}\"\n",
        "\n",
        "# ---- Pairs ----\n",
        "img_files = sorted(IMG_DIR.glob(\"*.png\"))\n",
        "\n",
        "pairs = []\n",
        "missing = 0\n",
        "for img_fp in img_files:\n",
        "    msk_fp = MSK_DIR / f\"{img_fp.stem}_mask.png\"\n",
        "    if not msk_fp.exists():\n",
        "        missing += 1\n",
        "        continue\n",
        "    pairs.append((img_fp, msk_fp))\n",
        "\n",
        "print(\"Num image tiles found:\", len(img_files))\n",
        "print(\"Paired tiles:\", len(pairs))\n",
        "print(\"Missing masks:\", missing)\n",
        "\n",
        "# ---- Split train/val/test ----\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "idx = list(range(len(pairs)))\n",
        "random.shuffle(idx)\n",
        "\n",
        "train_ratio, val_ratio = 0.80, 0.10\n",
        "n = len(idx)\n",
        "train_end = int(train_ratio * n)\n",
        "val_end   = int((train_ratio + val_ratio) * n)\n",
        "\n",
        "train_idx = idx[:train_end]\n",
        "val_idx   = idx[train_end:val_end]\n",
        "test_idx  = idx[val_end:]\n",
        "\n",
        "print(f\"Split sizes -> train: {len(train_idx)}, val: {len(val_idx)}, test: {len(test_idx)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE1bp34S7JKv",
        "outputId": "67e6b2b0-eae1-43b8-e59a-04e645eb281a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num image tiles found: 1215\n",
            "Paired tiles: 1215\n",
            "Missing masks: 0\n",
            "Split sizes -> train: 972, val: 121, test: 122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Class + Data Augmentations + Data Loader**"
      ],
      "metadata": {
        "id": "77NrFBy420F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Augmentations ----\n",
        "# Train gets augmentation; Val/Test do not (only normalize + to tensor)\n",
        "train_tf = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.10, rotate_limit=15, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),  # keeps your existing [0,1] scale logic simple\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "eval_tf = A.Compose([\n",
        "    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "\n",
        "# ---- Dataset ----\n",
        "class TileSegDataset(Dataset):\n",
        "    \"\"\"\n",
        "    pairs: list[(img_fp, msk_fp)]\n",
        "    indices: list[int] selecting which pairs belong to this split\n",
        "    transform: Albumentations Compose (expects image HWC, mask HW)\n",
        "    \"\"\"\n",
        "    def __init__(self, pairs, indices, transform=None):\n",
        "        self.pairs = pairs\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img_fp, msk_fp = self.pairs[self.indices[i]]\n",
        "\n",
        "        # RGB image uint8 (H,W,3)\n",
        "        image = np.array(Image.open(img_fp).convert(\"RGB\"), dtype=np.uint8)\n",
        "\n",
        "        # mask uint8 (H,W) with labels {0,1,2,3}\n",
        "        mask = np.array(Image.open(msk_fp), dtype=np.uint8)\n",
        "        if mask.ndim == 3:          # safety (if saved as RGB accidentally)\n",
        "            mask = mask[..., 0]\n",
        "\n",
        "        # apply augmentations (mask stays integer labels)\n",
        "        if self.transform is not None:\n",
        "            out = self.transform(image=image, mask=mask)\n",
        "            image = out[\"image\"]                # torch float32 (3,H,W)\n",
        "            mask  = out[\"mask\"].long()          # torch int64 (H,W)\n",
        "        else:\n",
        "            # fallback (no albumentations): manual conversion\n",
        "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "            mask  = torch.from_numpy(mask).long()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "# ---- Datasets + Loaders ----\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "train_ds = TileSegDataset(pairs, train_idx, transform=train_tf)\n",
        "val_ds   = TileSegDataset(pairs, val_idx,   transform=eval_tf)\n",
        "test_ds  = TileSegDataset(pairs, test_idx,  transform=eval_tf)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ---- Quick sanity check ----\n",
        "x, y = next(iter(train_loader))\n",
        "print(\"Batch images:\", x.shape, x.dtype)   # (B,3,512,512), float32\n",
        "print(\"Batch masks :\", y.shape, y.dtype)   # (B,512,512), int64\n",
        "print(\"Unique labels in batch:\", torch.unique(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJUSss167ZYZ",
        "outputId": "c35fb2d5-dbdd-40f2-e844-577ea7b61636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch images: torch.Size([4, 3, 512, 512]) torch.float32\n",
            "Batch masks : torch.Size([4, 512, 512]) torch.int64\n",
            "Unique labels in batch: tensor([0, 1, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics (F1,IoU,Precision/Recall per class, Pixel Accuracy)**"
      ],
      "metadata": {
        "id": "e7qDIcEi8zxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 4\n",
        "\n",
        "def build_metrics(device):\n",
        "    metrics = {\n",
        "        \"f1_macro\": MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\").to(device),\n",
        "        \"f1_per_class\": MulticlassF1Score(num_classes=NUM_CLASSES, average=None).to(device),\n",
        "\n",
        "        \"iou_macro\": MulticlassJaccardIndex(num_classes=NUM_CLASSES, average=\"macro\").to(device),\n",
        "        \"iou_per_class\": MulticlassJaccardIndex(num_classes=NUM_CLASSES, average=None).to(device),\n",
        "\n",
        "        \"prec_per_class\": MulticlassPrecision(num_classes=NUM_CLASSES, average=None).to(device),\n",
        "        \"rec_per_class\": MulticlassRecall(num_classes=NUM_CLASSES, average=None).to(device),\n",
        "\n",
        "        \"pixel_acc\": MulticlassAccuracy(num_classes=NUM_CLASSES, average=\"micro\").to(device),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "@torch.no_grad()\n",
        "def reset_metrics(metrics: dict):\n",
        "    for m in metrics.values():\n",
        "        m.reset()\n",
        "\n",
        "@torch.no_grad()\n",
        "def update_metrics(metrics: dict, preds: torch.Tensor, targets: torch.Tensor):\n",
        "    \"\"\"\n",
        "    preds: (B,H,W) int64\n",
        "    targets: (B,H,W) int64\n",
        "    \"\"\"\n",
        "    for m in metrics.values():\n",
        "        m.update(preds, targets)\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_metrics(metrics: dict):\n",
        "    out = {}\n",
        "    for k, m in metrics.items():\n",
        "        val = m.compute()\n",
        "        out[k] = val.detach().cpu()\n",
        "    return out"
      ],
      "metadata": {
        "id": "aPvyDU9h9DcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop(Epochs)**"
      ],
      "metadata": {
        "id": "iMc48gLr-niZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_one_epoch(model, loader, optimizer, device, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    metrics = build_metrics(device)\n",
        "    reset_metrics(metrics)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)   # (B,3,512,512)\n",
        "        y = y.to(device, non_blocking=True)   # (B,512,512)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(x)                 # expected (B,C,H,W)\n",
        "            # Some torchvision seg models return dict: {\"out\": ...}\n",
        "            if isinstance(logits, dict):\n",
        "                logits = logits[\"out\"]\n",
        "\n",
        "            loss = combined_loss(logits, y)\n",
        "\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)   # (B,H,W)\n",
        "        update_metrics(metrics, preds, y)\n",
        "\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    metrics_out = compute_metrics(metrics)\n",
        "    avg_loss = total_loss / max(1, n_batches)\n",
        "\n",
        "    return avg_loss, metrics_out, elapsed\n"
      ],
      "metadata": {
        "id": "3Nz-f_bz9eUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    model_name: str,\n",
        "    device,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    epochs=10,\n",
        "    results_dir=\"/content/drive/MyDrive/ResearchProject (1)/model_results\"\n",
        "):\n",
        "    results_dir = Path(results_dir)\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    history = []\n",
        "    best = {\n",
        "        \"epoch\": None,\n",
        "        \"val_f1_macro\": -1.0,\n",
        "        \"path_ckpt\": None,\n",
        "        \"path_metrics\": None\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        tr_loss, tr_metrics, tr_time = run_one_epoch(model, train_loader, optimizer, device, train=True)\n",
        "        va_loss, va_metrics, va_time = run_one_epoch(model, val_loader, optimizer, device, train=False)\n",
        "\n",
        "        # choose best by val mean F1 (macro)\n",
        "        val_f1 = float(va_metrics[\"f1_macro\"].item())\n",
        "\n",
        "        row = {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": tr_loss,\n",
        "            \"val_loss\": va_loss,\n",
        "            \"train_time_sec\": tr_time,\n",
        "            \"val_time_sec\": va_time,\n",
        "\n",
        "            \"train_f1_macro\": float(tr_metrics[\"f1_macro\"].item()),\n",
        "            \"val_f1_macro\": val_f1,\n",
        "\n",
        "            \"train_iou_macro\": float(tr_metrics[\"iou_macro\"].item()),\n",
        "            \"val_iou_macro\": float(va_metrics[\"iou_macro\"].item()),\n",
        "\n",
        "            \"val_pixel_acc\": float(va_metrics[\"pixel_acc\"].item()),\n",
        "\n",
        "            \"val_f1_per_class\": va_metrics[\"f1_per_class\"].tolist(),\n",
        "            \"val_iou_per_class\": va_metrics[\"iou_per_class\"].tolist(),\n",
        "            \"val_prec_per_class\": va_metrics[\"prec_per_class\"].tolist(),\n",
        "            \"val_rec_per_class\": va_metrics[\"rec_per_class\"].tolist(),\n",
        "        }\n",
        "        history.append(row)\n",
        "\n",
        "        print(\n",
        "            f\"[{model_name}] Epoch {epoch:02d}/{epochs} | \"\n",
        "            f\"val_f1={row['val_f1_macro']:.4f} val_iou={row['val_iou_macro']:.4f} \"\n",
        "            f\"loss={row['val_loss']:.4f} | \"\n",
        "            f\"time(train/val)={tr_time:.1f}s/{va_time:.1f}s\"\n",
        "        )\n",
        "\n",
        "        if val_f1 > best[\"val_f1_macro\"]:\n",
        "            best[\"val_f1_macro\"] = val_f1\n",
        "            best[\"epoch\"] = epoch\n",
        "\n",
        "            ckpt_path = results_dir / f\"{model_name}_best.pt\"\n",
        "            metrics_path = results_dir / f\"{model_name}_best_metrics.json\"\n",
        "\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            with open(metrics_path, \"w\") as f:\n",
        "                json.dump({\"best\": row, \"history\": history}, f, indent=2)\n",
        "\n",
        "            best[\"path_ckpt\"] = str(ckpt_path)\n",
        "            best[\"path_metrics\"] = str(metrics_path)\n",
        "\n",
        "    print(\"\\nBest epoch:\", best[\"epoch\"], \"| Best val mean F1:\", best[\"val_f1_macro\"])\n",
        "    print(\"Saved checkpoint:\", best[\"path_ckpt\"])\n",
        "    print(\"Saved metrics:\", best[\"path_metrics\"])\n",
        "\n",
        "    return best, history\n"
      ],
      "metadata": {
        "id": "jjHUovXx_Aab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function (CE + Dice)**"
      ],
      "metadata": {
        "id": "Aj6mS24VC53B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "def dice_loss(logits, targets, num_classes=4, eps=1e-6):\n",
        "    \"\"\"\n",
        "    logits: (B, C, H, W)\n",
        "    targets: (B, H, W) with class labels\n",
        "    \"\"\"\n",
        "    probs = F.softmax(logits, dim=1)              # (B,C,H,W)\n",
        "    targets_oh = F.one_hot(targets, num_classes)  # (B,H,W,C)\n",
        "    targets_oh = targets_oh.permute(0, 3, 1, 2).float()\n",
        "\n",
        "    dims = (0, 2, 3)\n",
        "    intersection = torch.sum(probs * targets_oh, dims)\n",
        "    union = torch.sum(probs + targets_oh, dims)\n",
        "\n",
        "    dice = (2 * intersection + eps) / (union + eps)\n",
        "    return 1 - dice.mean()\n",
        "\n",
        "def combined_loss(logits, targets):\n",
        "    return ce_loss(logits, targets) + dice_loss(logits, targets)"
      ],
      "metadata": {
        "id": "bZkHkKOIBjW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**U-Net(SMP) Model**"
      ],
      "metadata": {
        "id": "Zu9TVTR5Akal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"FullUNet_23L\"\n",
        "\n",
        "ENCODER = \"resnet34\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"  # good default\n",
        "\n",
        "model_unet = smp.Unet(\n",
        "    encoder_name=ENCODER,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=3,\n",
        "    classes=4,\n",
        "    activation=None\n",
        ").to(DEVICE)\n",
        "\n",
        "print(\"Model:\", MODEL_NAME)\n",
        "print(\"Encoder:\", ENCODER)\n",
        "\n",
        "# ---- Train and save BEST epoch (by val mean F1) ----\n",
        "best_unet, hist_unet = train_model(\n",
        "    model=model_unet,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    model_name=MODEL_NAME,\n",
        "    device=DEVICE,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    epochs=10,\n",
        "    results_dir=\"/content/drive/MyDrive/ResearchProject (1)/model_results\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4rzq9sA_Y-m",
        "outputId": "cefd3e63-5df8-490f-a73c-6bc443ad43a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: FullUNet_23L\n",
            "Encoder: resnet34\n",
            "[FullUNet_23L] Epoch 01/10 | val_f1=0.6065 val_iou=0.5108 loss=1.0501 | time(train/val)=14.9s/1.8s\n",
            "[FullUNet_23L] Epoch 02/10 | val_f1=0.5336 val_iou=0.4338 loss=1.2259 | time(train/val)=14.0s/1.7s\n",
            "[FullUNet_23L] Epoch 03/10 | val_f1=0.5764 val_iou=0.4830 loss=0.9898 | time(train/val)=13.8s/1.8s\n",
            "[FullUNet_23L] Epoch 04/10 | val_f1=0.7262 val_iou=0.6129 loss=0.8165 | time(train/val)=14.3s/1.8s\n",
            "[FullUNet_23L] Epoch 05/10 | val_f1=0.7974 val_iou=0.6816 loss=0.7413 | time(train/val)=15.9s/1.8s\n",
            "[FullUNet_23L] Epoch 06/10 | val_f1=0.7798 val_iou=0.6675 loss=0.7330 | time(train/val)=16.0s/1.8s\n",
            "[FullUNet_23L] Epoch 07/10 | val_f1=0.7371 val_iou=0.6168 loss=0.7934 | time(train/val)=14.6s/1.7s\n",
            "[FullUNet_23L] Epoch 08/10 | val_f1=0.7973 val_iou=0.6840 loss=0.6991 | time(train/val)=13.9s/1.7s\n",
            "[FullUNet_23L] Epoch 09/10 | val_f1=0.7604 val_iou=0.6375 loss=0.7617 | time(train/val)=13.9s/1.8s\n",
            "[FullUNet_23L] Epoch 10/10 | val_f1=0.7089 val_iou=0.5948 loss=0.8060 | time(train/val)=14.1s/1.8s\n",
            "\n",
            "Best epoch: 5 | Best val mean F1: 0.7974153757095337\n",
            "Saved checkpoint: /content/drive/MyDrive/ResearchProject (1)/model_results/FullUNet_23L_best.pt\n",
            "Saved metrics: /content/drive/MyDrive/ResearchProject (1)/model_results/FullUNet_23L_best_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DeepLabV3-ResNet50 Model**"
      ],
      "metadata": {
        "id": "WfJJMuteTG43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"DeepLabV3_ResNet50\"\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# Load pretrained backbone weights\n",
        "model = torchvision.models.segmentation.deeplabv3_resnet50(\n",
        "    weights=torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT\n",
        ")\n",
        "\n",
        "# Replaced classifier head so output channels = num classes\n",
        "in_ch = model.classifier[-1].in_channels\n",
        "model.classifier[-1] = nn.Conv2d(in_ch, NUM_CLASSES, kernel_size=1)\n",
        "\n",
        "# If aux classifier exists, match it too (safe)\n",
        "if model.aux_classifier is not None:\n",
        "    in_ch_aux = model.aux_classifier[-1].in_channels\n",
        "    model.aux_classifier[-1] = nn.Conv2d(in_ch_aux, NUM_CLASSES, kernel_size=1)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(\"Model:\", MODEL_NAME)\n",
        "print(\"Output head channels:\", NUM_CLASSES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj1HvSIVQoi_",
        "outputId": "c1ee46ce-7820-4af9-e43e-263d7d1d7b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161M/161M [00:01<00:00, 162MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: DeepLabV3_ResNet50\n",
            "Output head channels: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train DeepLabV3-ResNet50 Model**"
      ],
      "metadata": {
        "id": "OsjRFzemTfrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_deeplab, hist_deeplab = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    model_name=MODEL_NAME,\n",
        "    device=DEVICE,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    epochs=10,\n",
        "    results_dir=\"/content/drive/MyDrive/ResearchProject (1)/model_results\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqV_Zp6HTnb4",
        "outputId": "67ca21ae-1913-4b52-b7be-4db3801a5c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DeepLabV3_ResNet50] Epoch 01/10 | val_f1=0.5693 val_iou=0.4734 loss=1.1078 | time(train/val)=630.4s/72.5s\n",
            "[DeepLabV3_ResNet50] Epoch 02/10 | val_f1=0.6750 val_iou=0.5803 loss=0.8432 | time(train/val)=24.2s/1.7s\n",
            "[DeepLabV3_ResNet50] Epoch 03/10 | val_f1=0.5671 val_iou=0.4651 loss=1.1047 | time(train/val)=28.1s/1.6s\n",
            "[DeepLabV3_ResNet50] Epoch 04/10 | val_f1=0.7805 val_iou=0.6594 loss=0.7797 | time(train/val)=24.3s/1.7s\n",
            "[DeepLabV3_ResNet50] Epoch 05/10 | val_f1=0.7585 val_iou=0.6296 loss=0.8537 | time(train/val)=28.1s/1.7s\n",
            "[DeepLabV3_ResNet50] Epoch 06/10 | val_f1=0.7083 val_iou=0.5641 loss=0.9607 | time(train/val)=24.3s/1.7s\n",
            "[DeepLabV3_ResNet50] Epoch 07/10 | val_f1=0.8105 val_iou=0.6943 loss=0.6937 | time(train/val)=24.5s/1.7s\n",
            "[DeepLabV3_ResNet50] Epoch 08/10 | val_f1=0.7248 val_iou=0.5793 loss=0.9011 | time(train/val)=28.3s/1.7s\n",
            "[DeepLabV3_ResNet50] Epoch 09/10 | val_f1=0.6324 val_iou=0.5001 loss=1.0434 | time(train/val)=24.4s/1.7s\n",
            "[DeepLabV3_ResNet50] Epoch 10/10 | val_f1=0.8052 val_iou=0.6910 loss=0.6558 | time(train/val)=24.3s/1.7s\n",
            "\n",
            "Best epoch: 7 | Best val mean F1: 0.810496985912323\n",
            "Saved checkpoint: /content/drive/MyDrive/ResearchProject (1)/model_results/DeepLabV3_ResNet50_best.pt\n",
            "Saved metrics: /content/drive/MyDrive/ResearchProject (1)/model_results/DeepLabV3_ResNet50_best_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**U-Net + SCSE (ResNet-34 encoder) Model**"
      ],
      "metadata": {
        "id": "8omlyxcbdgDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"UNet_SCSE_ResNet34\"\n",
        "ENCODER = \"resnet34\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "model_scas = smp.Unet(\n",
        "    encoder_name=ENCODER,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=3,\n",
        "    classes=4,\n",
        "    activation=None,\n",
        "    decoder_attention_type=\"scse\"  #attention\n",
        ").to(DEVICE)\n",
        "\n",
        "print(\"Model:\", MODEL_NAME)\n",
        "print(\"Encoder:\", ENCODER)\n",
        "print(\"Decoder attention:\", \"scse\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrrDP-uRfEN4",
        "outputId": "adb2c673-934b-4c04-f3a3-43103c448eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 229MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: UNet_SCSE_ResNet34\n",
            "Encoder: resnet34\n",
            "Decoder attention: scse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train U-Net + SCSE (ResNet-34 encoder)**"
      ],
      "metadata": {
        "id": "gly29BBweG1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_scas, hist_scas = train_model(\n",
        "    model=model_scas,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    model_name=MODEL_NAME,\n",
        "    device=DEVICE,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    epochs=10,\n",
        "    results_dir=\"/content/drive/MyDrive/ResearchProject (1)/model_results\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UrQEOhwd-yZ",
        "outputId": "bc381322-2805-4e1e-c3bb-38db134863bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNet_SCSE_ResNet34] Epoch 01/10 | val_f1=0.6051 val_iou=0.5125 loss=1.0524 | time(train/val)=15.7s/1.9s\n",
            "[UNet_SCSE_ResNet34] Epoch 02/10 | val_f1=0.6269 val_iou=0.5411 loss=0.9240 | time(train/val)=15.6s/1.8s\n",
            "[UNet_SCSE_ResNet34] Epoch 03/10 | val_f1=0.5980 val_iou=0.5056 loss=0.9414 | time(train/val)=17.4s/1.7s\n",
            "[UNet_SCSE_ResNet34] Epoch 04/10 | val_f1=0.6613 val_iou=0.5924 loss=0.7540 | time(train/val)=15.7s/1.8s\n",
            "[UNet_SCSE_ResNet34] Epoch 05/10 | val_f1=0.7093 val_iou=0.6050 loss=0.7941 | time(train/val)=17.6s/1.7s\n",
            "[UNet_SCSE_ResNet34] Epoch 06/10 | val_f1=0.7496 val_iou=0.6386 loss=0.7511 | time(train/val)=17.5s/1.7s\n",
            "[UNet_SCSE_ResNet34] Epoch 07/10 | val_f1=0.7669 val_iou=0.6460 loss=0.8039 | time(train/val)=17.4s/1.7s\n",
            "[UNet_SCSE_ResNet34] Epoch 08/10 | val_f1=0.7836 val_iou=0.6633 loss=0.7398 | time(train/val)=17.3s/1.7s\n",
            "[UNet_SCSE_ResNet34] Epoch 09/10 | val_f1=0.6568 val_iou=0.5417 loss=1.0166 | time(train/val)=17.3s/1.7s\n",
            "[UNet_SCSE_ResNet34] Epoch 10/10 | val_f1=0.7885 val_iou=0.6667 loss=0.7419 | time(train/val)=15.4s/1.7s\n",
            "\n",
            "Best epoch: 10 | Best val mean F1: 0.788501501083374\n",
            "Saved checkpoint: /content/drive/MyDrive/ResearchProject (1)/model_results/UNet_SCSE_ResNet34_best.pt\n",
            "Saved metrics: /content/drive/MyDrive/ResearchProject (1)/model_results/UNet_SCSE_ResNet34_best_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPANetFull (ResNet50 backbone) Model**"
      ],
      "metadata": {
        "id": "9iTA1esUfT5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SPANetFull Model (ResNet50 backbone) + Train\n",
        "# ============================================================\n",
        "\n",
        "MODEL_NAME = \"SPANetFull_ResNet50\"\n",
        "print(\"Model:\", MODEL_NAME)\n",
        "\n",
        "# ----------------------------\n",
        "# ResNet50 encoder (low + high)\n",
        "# ----------------------------\n",
        "class ResNet50Encoder(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = ResNet50_Weights.DEFAULT if pretrained else None\n",
        "        base = resnet50(weights=weights)\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            base.conv1,\n",
        "            base.bn1,\n",
        "            base.relu,\n",
        "            base.maxpool,   # -> H/4, W/4\n",
        "        )\n",
        "        self.layer1 = base.layer1   # -> H/4,  W/4,  C=256\n",
        "        self.layer2 = base.layer2   # -> H/8,  W/8,  C=512   (low)\n",
        "        self.layer3 = base.layer3   # -> H/16, W/16, C=1024\n",
        "        self.layer4 = base.layer4   # -> H/32, W/32, C=2048  (high)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        low = self.layer2(x)         # (B,512,H/8,W/8)\n",
        "        x = self.layer3(low)\n",
        "        high = self.layer4(x)        # (B,2048,H/32,W/32)\n",
        "        return low, high\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# SPAM block (successive pooling attention)\n",
        "# ---------------------------------------------\n",
        "class SPAMBlock(nn.Module):\n",
        "    def __init__(self, in_channels, pool_sizes=(1, 2, 4), reduction=4):\n",
        "        super().__init__()\n",
        "        self.pool_sizes = pool_sizes\n",
        "        mid_channels = max(in_channels // reduction, 1)\n",
        "\n",
        "        self.conv_reduce = nn.Conv2d(in_channels * len(pool_sizes), mid_channels, kernel_size=1, bias=False)\n",
        "        self.bn_reduce = nn.BatchNorm2d(mid_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv_attn = nn.Conv2d(mid_channels, in_channels, kernel_size=1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        pooled_list = []\n",
        "        for ps in self.pool_sizes:\n",
        "            p = F.adaptive_avg_pool2d(x, output_size=(ps, ps))\n",
        "            p = F.interpolate(p, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
        "            pooled_list.append(p)\n",
        "\n",
        "        multi_scale = torch.cat(pooled_list, dim=1)              # (B, C*len(pool_sizes), H, W)\n",
        "        h = self.relu(self.bn_reduce(self.conv_reduce(multi_scale)))\n",
        "        attn = self.sigmoid(self.conv_attn(h))                   # (B, C, H, W)\n",
        "        return x * attn\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Feature Fusion Module (high -> gate low)\n",
        "# ---------------------------------------------\n",
        "class FeatureFusionModule(nn.Module):\n",
        "    def __init__(self, low_channels, high_channels):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        mid_channels = max(low_channels // 4, 1)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(high_channels, mid_channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(mid_channels, low_channels, kernel_size=1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, low, high):\n",
        "        B, C_l, H_l, W_l = low.shape\n",
        "\n",
        "        h = self.pool(high)                     # (B, C_h, 1, 1)\n",
        "        h = self.relu(self.bn1(self.conv1(h)))  # (B, mid, 1, 1)\n",
        "        h = self.sigmoid(self.conv2(h))         # (B, C_l, 1, 1)\n",
        "\n",
        "        attn = h.expand(-1, -1, H_l, W_l)\n",
        "        fused = low * attn\n",
        "        return fused\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Full SPANet-style segmentation network\n",
        "# ---------------------------------------------\n",
        "class SPANetFull(nn.Module):\n",
        "    def __init__(self, num_classes=4, pretrained_backbone=True):\n",
        "        super().__init__()\n",
        "        self.encoder = ResNet50Encoder(pretrained=pretrained_backbone)\n",
        "\n",
        "        self.low_ch = 512\n",
        "        self.high_ch = 2048\n",
        "\n",
        "        self.spam_low = SPAMBlock(self.low_ch)\n",
        "        self.spam_high = SPAMBlock(self.high_ch)\n",
        "\n",
        "        self.ffm = FeatureFusionModule(self.low_ch, self.high_ch)\n",
        "\n",
        "        # simple decoder\n",
        "        self.dec_conv1 = nn.Conv2d(self.low_ch, 256, kernel_size=3, padding=1)\n",
        "        self.dec_bn1 = nn.BatchNorm2d(256)\n",
        "        self.dec_conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.dec_bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.classifier = nn.Conv2d(128, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = x.shape[-2:]\n",
        "        low, high = self.encoder(x)\n",
        "\n",
        "        low_enh = self.spam_low(low)\n",
        "        high_enh = self.spam_high(high)\n",
        "\n",
        "        fused_low = self.ffm(low_enh, high_enh)         # (B,512,H/8,W/8)\n",
        "\n",
        "        y = self.dec_bn1(F.relu(self.dec_conv1(fused_low)))\n",
        "        y = F.interpolate(y, scale_factor=2, mode=\"bilinear\", align_corners=False)  # -> H/4\n",
        "\n",
        "        y = self.dec_bn2(F.relu(self.dec_conv2(y)))\n",
        "        y = F.interpolate(y, size=(H, W), mode=\"bilinear\", align_corners=False)     # -> H,W\n",
        "\n",
        "        logits = self.classifier(y)  # (B, num_classes, H, W)\n",
        "        return logits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbZi3VuBfVYE",
        "outputId": "b6f08f53-2c4f-4d4a-b9fe-4ae63ce5239c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: SPANetFull_ResNet50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train SPANetFull (ResNet50 backbone) Model**"
      ],
      "metadata": {
        "id": "oGMZ8Hm8faI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_spanet = SPANetFull(num_classes=4, pretrained_backbone=True).to(DEVICE)\n",
        "\n",
        "best_spanet, hist_spanet = train_model(\n",
        "    model=model_spanet,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    model_name=MODEL_NAME,\n",
        "    device=DEVICE,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    epochs=10,\n",
        "    results_dir=\"/content/drive/MyDrive/ResearchProject (1)/model_results\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af78pNvhfdx7",
        "outputId": "d2a290af-4f95-4549-caff-cc818b6d2d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 240MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SPANetFull_ResNet50] Epoch 01/10 | val_f1=0.7411 val_iou=0.6224 loss=0.9439 | time(train/val)=26.6s/1.8s\n",
            "[SPANetFull_ResNet50] Epoch 02/10 | val_f1=0.6191 val_iou=0.5049 loss=1.0190 | time(train/val)=26.4s/1.8s\n",
            "[SPANetFull_ResNet50] Epoch 03/10 | val_f1=0.7708 val_iou=0.6572 loss=0.8157 | time(train/val)=26.2s/1.8s\n",
            "[SPANetFull_ResNet50] Epoch 04/10 | val_f1=0.6364 val_iou=0.5196 loss=1.0142 | time(train/val)=28.7s/1.7s\n",
            "[SPANetFull_ResNet50] Epoch 05/10 | val_f1=0.7829 val_iou=0.6647 loss=0.8126 | time(train/val)=26.4s/1.7s\n",
            "[SPANetFull_ResNet50] Epoch 06/10 | val_f1=0.6893 val_iou=0.5709 loss=0.9100 | time(train/val)=28.5s/1.7s\n",
            "[SPANetFull_ResNet50] Epoch 07/10 | val_f1=0.7318 val_iou=0.6152 loss=0.8365 | time(train/val)=26.4s/1.7s\n",
            "[SPANetFull_ResNet50] Epoch 08/10 | val_f1=0.7837 val_iou=0.6579 loss=0.8530 | time(train/val)=26.2s/1.7s\n",
            "[SPANetFull_ResNet50] Epoch 09/10 | val_f1=0.7901 val_iou=0.6755 loss=0.7504 | time(train/val)=28.6s/1.7s\n",
            "[SPANetFull_ResNet50] Epoch 10/10 | val_f1=0.8272 val_iou=0.7188 loss=0.6860 | time(train/val)=28.8s/1.7s\n",
            "\n",
            "Best epoch: 10 | Best val mean F1: 0.8271860480308533\n",
            "Saved checkpoint: /content/drive/MyDrive/ResearchProject (1)/model_results/SPANetFull_ResNet50_best.pt\n",
            "Saved metrics: /content/drive/MyDrive/ResearchProject (1)/model_results/SPANetFull_ResNet50_best_metrics.json\n"
          ]
        }
      ]
    }
  ]
}